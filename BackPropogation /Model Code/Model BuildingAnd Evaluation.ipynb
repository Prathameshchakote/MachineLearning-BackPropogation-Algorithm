{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74127585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290a490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def get_dataset(file_name, training_count, shuffle=True):\n",
    "    traing_data = pd.read_csv(file_name)\n",
    "    data = traing_data.to_numpy()\n",
    "\n",
    "    # Shuffle the data if specified\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    # Extract features (X) and labels (Y) from the data\n",
    "    X = data[:, 1:]\n",
    "    # Limit training data count if specified\n",
    "    if training_count > 0:\n",
    "        X = X[:training_count, :]\n",
    "\n",
    "    # Add bias term to features\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    # Normalize features\n",
    "    X_with_bias = X_with_bias / 255.0\n",
    "\n",
    "    # Extract labels (Y) from the data\n",
    "    Y = data[:, 0].reshape(-1, 1)\n",
    "    # Limit training data count if specified\n",
    "    if training_count > 0:\n",
    "        Y = Y[:training_count, 0].reshape(-1, 1)\n",
    "    \n",
    "    # Encode labels using one-hot encoding\n",
    "    num_classes = 10\n",
    "    Y_onehotEncoded = onehot_encode(Y)\n",
    "\n",
    "    return X_with_bias, Y_onehotEncoded\n",
    "\n",
    "# One-hot encoding function\n",
    "def onehot_encode(T, num_classes=10, soft_weights=True):\n",
    "    Y_gt = np.zeros((T.shape[0], num_classes))\n",
    "    \n",
    "    if soft_weights:\n",
    "        # Soft weights: assign 0.1 to all classes\n",
    "        Y_gt += 0.1\n",
    "        # Set the value to 0.9 for the true class\n",
    "        Y_gt[np.arange(T.shape[0]), T[:, 0]] = 0.9\n",
    "    else:\n",
    "        # Hard weights: assign 1 to the true class\n",
    "        Y_gt[np.arange(T.shape[0]), T[:, 0]] = 1\n",
    "    return Y_gt\n",
    "\n",
    "# Function to generate random weights\n",
    "def generate_wt(rows, columns):\n",
    "    l = []\n",
    "    \n",
    "    # Generate random weights within the range [-0.5, 0.5]\n",
    "    for i in range(rows * columns):\n",
    "        l.append(np.random.uniform(-0.5, 0.5))\n",
    "\n",
    "    return np.array(l).reshape(rows, columns)\n",
    "\n",
    "# Function for forward propagation\n",
    "def forwardPropagation(X, layer_1_wts, layer_2_wts):\n",
    "    # Compute the activation of the hidden layer\n",
    "    hidden_layer = np.matmul(X, layer_1_wts)\n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "    h = sigmoid_vectorized(hidden_layer).reshape(1, -1)  # Activation of hidden node\n",
    "\n",
    "    # Add bias term to the hidden layer\n",
    "    h_layer = np.hstack((np.ones((h.shape[0], 1)), h))\n",
    "\n",
    "    # Compute the output layer\n",
    "    output_layer = np.dot(layer_2_wts, h_layer.T)\n",
    "    o_layer = sigmoid_vectorized(output_layer)\n",
    "\n",
    "    return h_layer, o_layer\n",
    "\n",
    "# Function to calculate gradients\n",
    "def calculateGradient(learningRate, X, output_error, hidden_error, acti_out, act_hid):\n",
    "    # Compute the gradient for the output layer weights\n",
    "    grad_out = learningRate * output_error * act_hid\n",
    "\n",
    "    # Remove bias term from the hidden error\n",
    "    hidden_error = hidden_error.T[:, 1:]\n",
    "\n",
    "    # Reshape input data\n",
    "    X = X.reshape(1, -1)\n",
    "\n",
    "    # Compute the gradient for the hidden layer weights\n",
    "    grad_hid = learningRate * X.T * hidden_error\n",
    "\n",
    "    return grad_out, grad_hid\n",
    "\n",
    "# Function to calculate errors during backpropagation\n",
    "def calculateError2(h_layer, o_layer, y, hidden_to_output_Weight):\n",
    "    # Reshape label data\n",
    "    y_new = y.reshape(-1, 1)\n",
    "\n",
    "    # Compute error for the output layer\n",
    "    error_output_layer = o_layer * (1 - o_layer) * (y_new - o_layer)\n",
    "\n",
    "    # Compute error for the hidden layer\n",
    "    tt = np.dot(hidden_to_output_Weight.T, error_output_layer)\n",
    "    error_hidlayer = h_layer * (1 - h_layer) * tt.T\n",
    "\n",
    "    return error_output_layer, error_hidlayer.T\n",
    "\n",
    "# Function for prediction\n",
    "def predict(X, layer_1_wts, layer_2_wts):\n",
    "    # Compute the activation of the hidden layer\n",
    "    hidden_layer = np.dot(X, layer_1_wts)\n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "    h = sigmoid_vectorized(hidden_layer)  # Activation of hidden node\n",
    "\n",
    "    # Add bias term to the hidden layer\n",
    "    h_layer = np.hstack((np.ones((h.shape[0], 1)), h))\n",
    "\n",
    "    # Compute the output layer\n",
    "    output_layer = np.dot(layer_2_wts, h_layer.T)\n",
    "    o_layer = sigmoid_vectorized(output_layer)\n",
    "\n",
    "    return h_layer, o_layer\n",
    "\n",
    "# Function to count rows with same argmax values between two arrays\n",
    "def count_same_argmax_rows(arr1, arr2):\n",
    "    # Find argmax values for each array\n",
    "    argmax_arr1 = np.argmax(arr1, axis=1)\n",
    "    argmax_arr2 = np.argmax(arr2, axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(argmax_arr1, argmax_arr2)\n",
    "\n",
    "    # Count rows with same argmax values\n",
    "    same_argmax_rows = np.sum(argmax_arr1 == argmax_arr2)\n",
    "    return same_argmax_rows, conf_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the neural network\n",
    "def train(learning_rate, momentum, num_epoch, num_neuron_in_hiddenLayer, training_count):\n",
    "    # Initialize parameters\n",
    "    hidden_layers_count = num_neuron_in_hiddenLayer\n",
    "    l_rate = learning_rate\n",
    "    momentum = momentum\n",
    "    \n",
    "    # Load training data\n",
    "    X, Y_onehotEncoded = get_dataset(\"/content/drive/MyDrive/MachineLearning/mnist_train.csv\", training_count)\n",
    "\n",
    "    # Initialize weights\n",
    "    hidden_wt = generate_wt(hidden_layers_count, X.shape[1])\n",
    "    output_wt = generate_wt(Y_onehotEncoded.shape[1], hidden_layers_count + 1)\n",
    "\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        # Load training data for each epoch\n",
    "        X, y = get_dataset(\"/content/drive/MyDrive/MachineLearning/mnist_train.csv\", training_count)\n",
    "        total = 0\n",
    "        print(\"epoch started := \", epoch)\n",
    "        \n",
    "        # Initialize gradient for weights\n",
    "        prev_hidden_wt_grad = np.zeros((785, hidden_layers_count))\n",
    "        prev_output_wt_grad = np.zeros((10, hidden_layers_count + 1))\n",
    "        \n",
    "        # Iterate over each sample in the training data\n",
    "        for i in range(X.shape[0]):\n",
    "\n",
    "            # Forward Propagation\n",
    "            h_layer, o_layer = forwardPropogation(X[i, :], hidden_wt.T, output_wt)\n",
    "\n",
    "            # Calculate Error\n",
    "            error_output_layer, error_hidden_layer = calculateError2(h_layer, o_layer, y[i, :], output_wt)\n",
    "            \n",
    "            # Calculate Gradient\n",
    "            grad_out, grad_hid = calculateGradient(l_rate, X[i, :], error_output_layer, error_hidden_layer, o_layer, h_layer)\n",
    "\n",
    "            # Calculate new weights\n",
    "            new_output_wt = output_wt + grad_out + momentum * prev_output_wt_grad\n",
    "            new_hidden_wt = hidden_wt + grad_hid.T + momentum * prev_hidden_wt_grad.T\n",
    "\n",
    "            # Update weights\n",
    "            output_wt = new_output_wt\n",
    "            hidden_wt = new_hidden_wt\n",
    "\n",
    "            # Update gradients\n",
    "            prev_output_wt_grad = grad_out\n",
    "            prev_hidden_wt_grad = grad_hid\n",
    "\n",
    "            # Calculate total accuracy\n",
    "            total = total + (np.argmax(o_layer) == np.argmax(y[i, :]))\n",
    "\n",
    "        # Append training accuracy for the epoch\n",
    "        train_accuracy.append(total * 100 / X.shape[0])\n",
    "\n",
    "        # Load test data\n",
    "        X_test, y_test = get_dataset(\"mnist_test.csv\", training_count=0)\n",
    "\n",
    "        # Predict output for Test Data\n",
    "        _, y_test_predict = predict(X_test, hidden_wt.T, output_wt)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        total_test, confusion_matrix = count_same_argmax_rows(y_test_predict.T, y_test)\n",
    "        test_accuracy.append(total_test * 100 / X_test.shape[0])\n",
    "\n",
    "        print(\"accuracy_train. \", train_accuracy)\n",
    "        print(\"accuracy_test. \", test_accuracy)\n",
    "\n",
    "    return train_accuracy, test_accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24b12f",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf66b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Experiment 1\n",
    "def experiment1():\n",
    "    print(\"experiment1 started\")\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.9\n",
    "    epoch = 50\n",
    "    training_count = 0   # Training count 0 means consider all data for training\n",
    "    \n",
    "    # Iterate over different hidden units\n",
    "    for hid_unit in [20, 50, 100]:\n",
    "        # Train the model and get accuracy metrics\n",
    "        train_accuracy, test_accuracy, confusion_matrix = train(learning_rate, momentum, epoch, hid_unit, training_count)\n",
    "\n",
    "        epoch_numbers = [i for i in range(0, epoch)]\n",
    "        \n",
    "        # Plot the first line graph for training accuracy\n",
    "        sns.lineplot(x=epoch_numbers, y=train_accuracy, label='Train Accuracy')\n",
    "\n",
    "        # Plot the second line graph on the same graph for validation accuracy\n",
    "        sns.lineplot(x=epoch_numbers, y=test_accuracy, label='Validation Accuracy')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Number Of epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Number Of epoch vs Accuracy')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix for MNIST Dataset')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70aabb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Experiment 2\n",
    "def experiment2():\n",
    "    print(\"experiment2 started\")\n",
    "    learning_rate = 0.1\n",
    "    epoch = 50\n",
    "    hid_unit = 100\n",
    "    training_count = 0   # Training count 0 means consider all data for training\n",
    "    \n",
    "    # Iterate over different momentum values\n",
    "    for momentum in [0, 0.25, 0.5]:\n",
    "        # Train the model and get accuracy metrics\n",
    "        train_accuracy, test_accuracy, confusion_matrix = train(learning_rate, momentum, epoch, hid_unit, training_count)\n",
    "\n",
    "        epoch_numbers = [i for i in range(0, epoch)]\n",
    "        \n",
    "        # Plot the first line graph for training accuracy\n",
    "        sns.lineplot(x=epoch_numbers, y=train_accuracy, label='Train Accuracy')\n",
    "\n",
    "        # Plot the second line graph on the same graph for validation accuracy\n",
    "        sns.lineplot(x=epoch_numbers, y=test_accuracy, label='Validation Accuracy')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Number Of epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f\"Number Of epoch vs Accuracy for momentum {momentum}\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title(f\"Confusion Matrix for momentum {momentum}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca84b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Experiment 3\n",
    "def experiment3():\n",
    "    print(\"experiment3 started\")\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.9\n",
    "    epoch = 50\n",
    "    hid_unit = 100\n",
    "\n",
    "    # Iterate over different training counts\n",
    "    for training_count in [15000, 30000]:\n",
    "        # Train the model and get accuracy metrics\n",
    "        train_accuracy, test_accuracy, confusion_matrix = train(learning_rate, momentum, epoch, hid_unit, training_count)\n",
    "\n",
    "        # Example data\n",
    "        epoch_numbers = [i for i in range(0, epoch)]\n",
    "        \n",
    "        # Plot the first line graph for training accuracy\n",
    "        sns.lineplot(x=epoch_numbers, y=train_accuracy, label='Train Accuracy')\n",
    "\n",
    "        # Plot the second line graph on the same graph for validation accuracy\n",
    "        sns.lineplot(x=epoch_numbers, y=test_accuracy, label='Validation Accuracy')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Number Of epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Number Of epoch vs Accuracy')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix for MNIST Dataset')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774cb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1()\n",
    "experiment2()\n",
    "experiment3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
